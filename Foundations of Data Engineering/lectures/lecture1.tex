\section{File formats}
Files can have a huge variety of formats, when in doubt the \texttt{file} command can be useful.

CSV are plain text files containing rows of data separated by a comma, in a simple format with customizable separator. It requires quoting of separators within strings.

XML is a text format encoding of semi structured data, better standardized than CSV but not human writable. It is suitable for nested objects and allows advanced features, yet it is very verbose and its use is declining.

JSON is similar to XML although much simpler and less verbose, easy to write. Its popularity is growing. 

\subsection{Command line tools}
Text formats are overall well-known and can be manipulated with command line tools, a very powerful instrument to perform preliminary analysis:
\begin{itemize}
	\item \texttt{cat} shows the content of one or multiple files (works with piped input as well);
	\item \texttt{zcat} is \texttt{cat} for compressed files;
	\item \texttt{less} allows paging (chopping long lines);
	\item \texttt{grep} is useful to search text (regex goes between quotes) with options such as file formats, lines not matching and case insensitiveness;
	\item \texttt{sort} to (merge) sort even large output;
	\item \texttt{uniq} handles duplicates;
	\item \texttt{tail} and \texttt{less} display suffixes and prefixes;
	\item \texttt{sed} to edit text, match and replace characters (in-place update using the same file);
	\item \texttt{join} to combine sorted files according to a common field;
	\item \texttt{awk} (followed by a \texttt{BEGIN-END} block) executes a program for every line of input, such as average or sum;
	\item \dots
\end{itemize}

\subsection{Performance spectrum}
Analyzing the performance is relevant to understand whether an operation is taking too long, and comparing it according to time and input size.

The performance spectrum involves many variables: theoretical limits, programming tools and methods, or hardware (clustering, scaling). Real input is complex, yet just analyzing simple text queries can be highly explicative.

Performance when manipulating text completely ignores CPU costs, which are not important for disks but very relevant for reading data from DRAM: this option is one of the fastest, followed by SSD.

A good way to optimize code is memory mapping: files are saved in the address space of the program, and are treated as arrays being accessed in the same way as dynamic memory. 

Another useful way consists in using block operations. Search can be performed for instance through the encoding of special characters, since there is no way to split the data just by directly scanning the memory.

Optimisation of mathematical operations is done using several strategies:
\begin{itemize}
	\item By the compiler, choosing fastest registry operations (64-bit is faster than 32);
	\item $a / c = a * (2^{64} / c) >> 64$;
	\item \texttt{const} instead of \texttt{constexpr};
\end{itemize}



