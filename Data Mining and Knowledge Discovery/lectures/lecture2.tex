\section{Signal processing}
Continuous signals are sampled starting from an original sinusoidal shape to obtain a set of discrete values which approximately represent the function. 

Shannon's sampling theorem states that having band $s(t)$ and Fourier spectrum $\abs{s(j2\pi f)} = 0$ for $\abs{f} > f_{max}$, the sampling time $T$ must be $T_s < \frac{1}{2f_{max}}$ (Nyquist condition) to completely reconstruct the signal. 

The theorem assumes infinite number of observations, not really possible in practice: arbitrary values are then chosen for sampling time. 

Quantization is the process of constraining an input from continue values to discrete, irreversible because of the quantization error (information gets lost in the process). A good step size needs to be chosen according to scale and placement of samples. 

\section{Data preprocessing}
Data preprocessing is useful to identify individual errors and perform operations such as merging, normalization and modeling.

The first assumption to make is for errors to be random, due to transmission and measurement, so that they can be treated as additive noise. 

Outliers can be caused by processing errors (wrong or permuted data), therefore if there are single incorrect values they may be removed - although there is no certain way to ensure a sample has been subject of mistakes. 

Detecting outliers can be done using statistical measures (sigma rule) or imposing stronger criteria on range and distribution. 

Errors can be replaced with mean, median, minimum or maximum of the valid feature data $x^i$, or take the value of the nearest neighbor (having the smallest norm). Examples of error handling:
\begin{itemize}
	\item Linear interpolation for equidistant time series;
	\item Linear interpolation for non-equidistant time series;
	\item Nonlinear interpolation (splines);
	\item Estimation by regression;
	\item Filtering;
	\item Outlier removal (of the complete vector);
	\item Feature removal.
\end{itemize} 


