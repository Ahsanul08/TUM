\section{Data Mining process}

\subsection{Data sources}
Examples of data sources are: 
\begin{itemize}
	\item Industrial process data, with many uses such as optimizing processes and predicting, but requires access and management of large quantities of information;
	\item Business data, mainly generated by shopping transactions, and having applications of market basked analysis and customer segmentation;
	\item Text (structured data),subject of natural language processing to understand keywords and meaning of documents;
	\item Image data, important source in terms of amount and especially targeted by deep learning;
	\item Biomedical data, such as genome and laboratory data.
\end{itemize}

Data can be projected onto two-dimensional planes, assigning visualization variables (color, height) to characteristics of data.

\subsection{Definitions}
Data mining is the process concerning the extraction of knowledge from data. Knowledge is defined as interesting patterns, therefore general, non trivial and comprehensive information.

Knowledge discovery consists, in fact, in preprocessing the a priori knowledge and extracting more, followed by evaluation (postprocessing) of the obtained data. The process can be summarised as follows:
\begin{enumerate}
	\item Preparation: collecting data, planning, generating and selecting features. Generally, this happens before involving data analysts;
	\item Preprocessing, all the operations of normalization, cleansing, filtering and correction;
	\item Analysis, the whole process of visualization, clustering, correlation etc.;
	\item Postprocessing, the interpretation of results using a systematic approach with related documentation.
\end{enumerate}

Patterns aren't always obvious in the beginning: the application of computer systems to the analysis (data analytics) of large datasets can offer great support to decisions.

Getting feedback must involve expert in related areas, such as statistics, pattern recognition, machine learning and operations research.

\section{Data and relations}
One of the most famous datasets is Iris, using $n = 150$ vectors of dimension $p = 4$.
It contains 50 instances for each of 3 classes, and 4 components representing characteristics of the flower. This collection is widely used for classification, and has all the properties of modern datasets. 

Typical questions to ask are whether the data is correct (rounding errors, false assignments) and the correlation between two variables, to have a better identification of each type with its features. 

Numerical data can have very different meanings: according to those and their type (usually nominal values with their occurrence), they can be treated in many ways. Examples of scales are:
\begin{itemize}
	\item Ratio $(\cdot, /)$ to detect the generalized mean, which allows to add an exponent $\alpha \in \mathbb{R}$ to the mean summation;
	\item Interval $(+, -)$ to detect the mean and compare it, getting information on outliers;
	\item Ordinal $(<, >)$ to detect the median by ordering them;
	\item Nominal $(=, \neq)$, to detect the mode since the only operation to perform is equality counting the number of occurrences in a bar chart.
\end{itemize}

After understanding the scale, there is a systematic arrangement of data $X = \{x_1, \dots, x_n\} \subset \mathbb{R}^p$ in a matrix. It has to be written in a way in which each row represents one observation, and each column represents a feature.

Object data has the form $O = \{o_1, \dots, o_n\}$ while relational data is in the form of a matrix $R$. There are infinitely relationships which can be specified, yet two of the most important ones are similarity and dissimilarity. The feature vectors have to be the same for two objects to determine similarity. 

The measure can be computed using the Euclidean distance definition, which has three main properties:
\begin{enumerate}
	\item $d(x, y) = d(y, x)$;
	\item $d(x, y) = 0 \leftrightarrow x = y$;
	\item $d(x, z) \leq d(x, y) = d(y, z)$.
\end{enumerate}
The third one is known as triangle inequality and is particularly useful for Euclidean mapping. It is possible to apply other functions.

The norm $d(x, y) = \norm{x - y} = \norm{y - x} = d(y, x)$ (not to be confused with the hyperbolic norm, a product) is used to measure dissimilarity and has the following properties:
\begin{enumerate}
	\item $\norm{x} + 0 \leftrightarrow x = (0 \dots, 0)$;
	\item $\norm{a \cdot x} = \norm{a} \cdot \norm{x}$;
	\item $\norm{x + y} \leq \norm{x} + \norm{y}$.
\end{enumerate}

Matrices have different kinds of norms: Euclidean, diagonal, Mahalanobis, Minkowski and many more. The usual method is Euclidean. 

Similarity is defined through formulas such as Dice and Jaccard or proximity measures:
\begin{enumerate}
	\item $s(x, y) = s(y, x)$;
	\item $s(x, y) \leq s(x, x)$;
	\item $s(x, y) \geq 0$.
\end{enumerate}
Similarity measures are used to compare internet pages according to the words in them, and give a general idea of the frequency distribution over nominal data. Vectors can be multiplied to obtain similarity as well, yet output needs to be normalized. 

Distances for sequences and text are symbol distance, Hamming and edit (Levenshtein). The con with Hamming is the need for two strings to have the same size. Edit distance is a good way to solve this problem, minimizing the number of operations to get the maximum similarity. 


