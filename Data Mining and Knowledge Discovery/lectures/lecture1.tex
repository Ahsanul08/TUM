\section{Data Mining Process}

\subsection{Data Sources}
Examples of data sources are: 
\begin{itemize}
	\item Industrial process data, with many uses such as optimizing processes and predicting, but requires access and management of large quantities of information;
	\item Business data, mainly generated by shopping transactions, and having applications of market basked analysis and customer segmentation;
	\item Text (structured data),subject of natural language processing to understand keywords and meaning of documents;
	\item Image data, important source in terms of amount and especially targeted by deep learning;
	\item Biomedical data, such as genome and laboratory data.
\end{itemize}

Data can be projected onto two-dimensional planes, assigning visualization variables (color, height) to characteristics of data.

\subsection{Definitions}
Data mining is the process concerning the extraction of knowledge from data. Knowledge is defined as interesting patterns, therefore general, non trivial and comprehensive information.

Knowledge discovery consists, in fact, in preprocessing the a priori knowledge and extracting more, followed by evaluation (postprocessing) of the obtained data. The process can be summarised as follows:
\begin{enumerate}
	\item Preparation: collecting data, planning, generating and selecting features. Generally, this happens before involving data analysts;
	\item Preprocessing, all the operations of normalization, cleansing, filtering and correction;
	\item Analysis, the whole process of visualization, clustering, correlation etc.;
	\item Postprocessing, the interpretation of results using a systematic approach with related documentation.
\end{enumerate}

Patterns aren't always obvious in the beginning: the application of computer systems to the analysis (data analytics) of large datasets can offer great support to decisions.

Getting feedback must involve expert in related areas, such as statistics, pattern recognition, machine learning and operations research.

\section{Data and Relations}
One of the most famous datasets is Iris, using $n = 150$ vectors of dimension $p = 4$.
It contains 50 instances for each of 3 classes, and 4 components representing characteristics of the flower. This collection is widely used for classification, and has all the properties of modern datasets. 

Typical questions to ask are whether the data is correct (rounding errors, false assignments) and the correlation between two variables, to have a better identification of each type with its features. 

